"""Agent runner for Prean C (agent as controller).

æ­¤æ¨¡å—å¯åŠ¨ LangChain agentï¼ˆè‹¥å¯ç”¨ï¼‰å¹¶å°† Cheat Engine å·¥å…·æ³¨å†Œä¸ºå·¥å…·åº“ã€‚
è®¾è®¡ç›®æ ‡ï¼šç”Ÿäº§å°±ç»ªçš„æœ€å°æ§åˆ¶å™¨ï¼ŒåŒ…å«å®¡è®¡æ—¥å¿—ã€dry-runã€ç ´åæ€§å®¡æ‰¹æ£€æŸ¥ä¸é‡è¯•ç­–ç•¥ã€‚
"""
from __future__ import annotations

import json
import logging
import os
import time
from datetime import datetime
from typing import Any, Dict, List

from dotenv import load_dotenv

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools import Tool
from langchain.agents import create_agent
from langchain_ollama import ChatOllama

from .ollama_adapter import OllamaClient
from .ce_tools import make_langchain_tools, build_tool_metadata

load_dotenv()

LOG_LEVEL = os.environ.get("AGENT_LOG_LEVEL", "INFO")
logging.basicConfig(level=LOG_LEVEL)
logger = logging.getLogger("mcp_agent")

# åˆ›å»ºæ—¥å¿—ç›®å½•å’Œæ—¥å¿—æ–‡ä»¶
log_dir = "logs"
os.makedirs(log_dir, exist_ok=True)
log_filename = os.path.join(log_dir, f"agent_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

def log_raw_response(response_data):
    """å°†åŸå§‹å“åº”å†™å…¥æ—¥å¿—æ–‡ä»¶"""
    with open(log_filename, 'a', encoding='utf-8') as log_file:
        log_file.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')}] Raw Response:\n")
        log_file.write(json.dumps(response_data, ensure_ascii=False, indent=2))
        log_file.write("\n" + "="*80 + "\n")

class OllamaLLMWrapper:
    """ç®€å•çš„ LangChain LLM é€‚é…å™¨ï¼ˆå°½å¯èƒ½å…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰ã€‚

    åœ¨æœ‰ langchain çš„ç¯å¢ƒä¸‹å¯ä»¥ä½œä¸ºè‡ªå®šä¹‰ LLM ä¼ å…¥ agentã€‚
    """

    def __init__(self, client: OllamaClient, max_tokens: int = 512, temperature: float = 0.0):
        self.client = client
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate_text(self, prompt: str) -> str:
        resp = self.client.generate(prompt, max_tokens=self.max_tokens, temperature=self.temperature)
        return resp.get("text", "")


def run_with_langchain(prompt: str, ollama: OllamaClient, tools: List[Any], steps: int = 6) -> None:
    try:
        # åˆ›å»ºChatOllamaå®ä¾‹
        llm = ChatOllama(
            base_url=ollama.base_url,
            model=ollama.model,
            temperature=0.0
        )
        
        logger.info(f"Using Ollama model: {ollama.model} at {ollama.base_url}")

        # åˆ›å»ºå·¥å…·è°ƒç”¨ä»£ç†
        agent = create_agent(
            model=llm,
            tools=tools
        )

        logger.info("Starting agent (LangChain) for prompt: %s", prompt)
        result = agent.invoke({"input": prompt})
        logger.info("Agent finished. Result:\n%s", result)
    except Exception as e:
        logger.exception("LangChain integration failed, falling back to direct loop: %s", e)
        run_fallback_loop(prompt, ollama, tools, steps)


def run_fallback_loop(prompt: str, ollama: OllamaClient, tools_meta: List[Dict[str, Any]], max_steps: int = 6) -> None:
    """å½“æ²¡æœ‰ langchain æˆ–é›†æˆå¤±è´¥æ—¶ä½¿ç”¨çš„å›é€€è°ƒåº¦å™¨ã€‚

    åè®®çº¦å®šï¼šOLLAMA è¾“å‡ºåº”åŒ…å«å¯è§£æçš„å·¥å…·è°ƒç”¨ JSONï¼ˆè§ ollama_adapter.extract_tool_callï¼‰ã€‚
    """
    print(f"\nğŸ” å¼€å§‹åˆ†æè¯·æ±‚: {prompt}")
    print("="*60)
    
    logger.info("Starting fallback agent loop")
    current_prompt = prompt
    
    for step in range(max_steps):
        logger.info("LLM -> step %d", step + 1)
        print(f"\nğŸ”„ æ‰§è¡Œæ­¥éª¤ {step + 1}/{max_steps}")
        
        try:
            # æ·»åŠ è¶…æ—¶å¤„ç†
            start_time = time.time()
            resp = ollama.generate(current_prompt)
            elapsed = time.time() - start_time
            
            text = resp.get("text", "")
            raw = resp.get("raw", {})
            
            # è®°å½•åŸå§‹å“åº”åˆ°æ—¥å¿—æ–‡ä»¶
            log_raw_response(resp)
            
            # æå–æ¨¡å‹è¾“å‡ºå†…å®¹
            if isinstance(raw, dict):
                thinking = raw.get("thinking", "")
                response = raw.get("response", "")
                
                if thinking:
                    print(f"ğŸ’¡ æ¨¡å‹æ€è€ƒ: {thinking}")
                if response:
                    print(f"ğŸ’¬ æ¨¡å‹å›å¤: {response}")
            else:
                # å¦‚æœrawä¸æ˜¯å­—å…¸ï¼Œå°è¯•ä»textä¸­æå–å†…å®¹
                if text.strip():
                    print(f"ğŸ’¬ æ¨¡å‹å›å¤: {text}")
            
            # å°è¯•ä»textä¸­æå–å·¥å…·è°ƒç”¨
            tc = OllamaClient.extract_tool_call(text)
            if not tc:
                # å°è¯•ä»rawå“åº”ä¸­æå–
                if isinstance(raw, str):
                    import re
                    matches = re.findall(r'"response":"([^"]*)"', raw)
                    if matches:
                        extracted_text = matches[-1]
                        tc = OllamaClient.extract_tool_call(extracted_text)
            
            if not tc:
                logger.info("No tool_call detected. Final LLM output:\n%s", text)
                print(f"\nâœ… åˆ†æå®Œæˆï¼Œæœ€ç»ˆç»“æœ:")
                if isinstance(raw, dict) and "response" in raw:
                    print(f"{raw['response']}")
                else:
                    print(f"{text}")
                return

            name = tc.get("name")
            args = tc.get("args", {})
            logger.info("Parsed tool_call: %s %s", name, args)
            print(f"ğŸ”§ å·¥å…·è°ƒç”¨: {name}({args})")

            # æŸ¥æ‰¾å·¥å…·
            tool = next((t for t in tools_meta if t["name"] == name), None)
            if not tool:
                logger.error("Unknown tool requested: %s", name)
                print(f"âŒ é”™è¯¯: æœªçŸ¥å·¥å…· {name}")
                return

            # æ‰§è¡Œå·¥å…·
            try:
                raw_result = tool["func"](**args)
                logger.info("Tool %s result: %s", name, json.dumps(raw_result, ensure_ascii=False)[:1000])
                print(f"âœ… å·¥å…· {name} æ‰§è¡ŒæˆåŠŸ")
                print(f"ğŸ“Š ç»“æœæ‘˜è¦: {json.dumps(raw_result, ensure_ascii=False)[:500]}...")
                
                # å°†ç»“æœåé¦ˆç»™ LLM ä»¥è¿›è¡Œä¸‹æ­¥è®¡åˆ’
                current_prompt = f"Tool result for {name}: {json.dumps(raw_result, ensure_ascii=False)}\n\nNext:" + "\n"
            except PermissionError as e:
                logger.warning("Tool %s blocked by policy: %s", name, e)
                print(f"ğŸš« å·¥å…· {name} è¢«ç­–ç•¥é˜»æ­¢: {e}")
                return
            except Exception as e:
                logger.exception("Tool execution failed: %s", e)
                print(f"ğŸ’¥ å·¥å…· {name} æ‰§è¡Œå¤±è´¥: {e}")
                return
                
        except Exception as e:
            logger.exception("Error during step %d: %s", step + 1, e)
            print(f"ğŸ’¥ æ­¥éª¤ {step + 1} æ‰§è¡Œå‡ºé”™: {e}")
            return

    logger.info("Reached max steps (%d) without finalizing", max_steps)
    print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§æ­¥éª¤æ•° ({max_steps})ï¼Œåˆ†æç»“æŸ")


def run_interactive_mode(ollama: OllamaClient, tools: List[Any]):
    """äº¤äº’å¼æ¨¡å¼ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥å¤šä¸ªè¯·æ±‚"""
    print("ğŸ® æ¬¢è¿ä½¿ç”¨Cheat Engine AIä»£ç†ï¼")
    print("æ‚¨å¯ä»¥æå‡ºå†…å­˜åˆ†æç›¸å…³çš„è¯·æ±‚ï¼Œä¾‹å¦‚ï¼š")
    print("- 'æ‰¾åˆ°æ¸¸æˆçš„é‡‘å¸åœ°å€å¹¶åˆ†æå…¶ä¿®æ”¹å‡½æ•°'")
    print("- 'åˆ†æç©å®¶æ•°æ®ç»“æ„å¹¶æå–æ‰€æœ‰ç›¸å…³å­—æ®µ'")
    print("- 'æ‰¾åˆ°æ•°æ®åŒ…è§£å¯†å‡½æ•°å¹¶ç”Ÿæˆè§£å¯†è„šæœ¬'")
    print("- 'pingå¹¶å‘Šè¯‰æˆ‘Cheat Engineçš„ç‰ˆæœ¬ä¿¡æ¯'")
    print("- 'è¯»å–åœ°å€0x401000å¤„çš„å†…å­˜'")
    print("- è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº\n")
    
    print(f"ğŸ“‹ æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_filename}")
    
    while True:
        try:
            user_input = input("\nğŸ¯ è¯·è¾“å…¥æ‚¨çš„è¯·æ±‚ï¼ˆç¡®ä¿è¯·æ±‚å…·ä½“ä¸”æ˜ç¡®ï¼‰: ").strip()
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
                
            print(f"\nğŸš€ å¼€å§‹å¤„ç†è¯·æ±‚: {user_input}")
            run_with_langchain(user_input, ollama, tools)
            print("\n" + "="*60)
            
        except KeyboardInterrupt:
            print("\n\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            logger.exception("å¤„ç†ç”¨æˆ·è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: %s", e)
            print(f"ğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}")


def main():
    model_name = os.environ.get("OLLAMA_MODEL", "llama3.1:8b")
    ollama = OllamaClient(base_url=os.environ.get("OLLAMA_URL", "http://localhost:11434"),
                          model=model_name)
    tools = make_langchain_tools()
    prompt = os.environ.get("AGENT_PROMPT", "Perform analysis: ping and read memory at 0x401000")
    logger.info(f"Starting agent with model: {model_name}")
    
    # æ£€æŸ¥æ˜¯å¦è®¾ç½®äº†AGENT_PROMPTç¯å¢ƒå˜é‡ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™è¿›å…¥äº¤äº’æ¨¡å¼
    if "AGENT_PROMPT" in os.environ:
        print(f"ğŸ“‹ æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_filename}")
        print(f"ğŸš€ æ‰§è¡Œç¯å¢ƒå˜é‡è¯·æ±‚: {prompt}")
        run_with_langchain(prompt, ollama, tools)
    else:
        print("ğŸ“‹ æ—¥å¿—æ–‡ä»¶ä½ç½®: {log_filename}")
        print("ğŸ® æœªè®¾ç½®AGENT_PROMPTç¯å¢ƒå˜é‡ï¼Œå¯åŠ¨äº¤äº’æ¨¡å¼...")
        run_interactive_mode(ollama, tools)


if __name__ == "__main__":
    main()